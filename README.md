# Mnist_Dataset_Quantization


ğŸ§  MNIST Quantization with Keras
This project demonstrates how to apply Deep Learning Model Optimization techniques such as Pruning and Quantization using the MNIST dataset. It is built using TensorFlow/Keras and showcases different levels of model compression with accuracy benchmarking.

ğŸ“ Project Structure
       Quantization.ipynb: Main Jupyter notebook containing:

1. Model training on MNIST

2. Quantization-aware training (QAT)

3. Post-training quantization (PQT)

4. Model performance comparison

ğŸš€ Techniques Covered
âœ… Baseline model training

âœ… Post-training quantization (int8)

âœ… Quantization-aware training (QAT)

âœ… Size and latency comparisons

âœ… Accuracy and loss evaluation

ğŸ“Š Sample Results (Example)
Model	Accuracy	Size
Baseline (float32)	98.2%	1.2 MB
QAT	98.1%	300 KB
PQT	97.9%	250 KB

ğŸ›  Requirements
bash
Copy
Edit
pip install tensorflow numpy matplotlib
ğŸ§ª How to Run
Clone the repository

Open Quantization.ipynb in Jupyter or Colab

Run all cells step-by-step

ğŸ“¦ Future Plans
Add support for model export (.tflite)

Deploy compressed models to edge devices

Compare different datasets (Fashion MNIST, CIFAR-10)

ğŸ¤ Credits
Developed by: [Your Name]

Dataset: MNIST

Model optimization: TensorFlow Model Optimization Toolkit
